# main.py
import pandas as pd
import networkx as nx
import json
import os
import logging
from config import *
from data_loader import DataLoader
from data_processor import DataProcessor
from network_builder import NetworkBuilder

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def export_graph_for_echarts(G: nx.Graph, path: str = "graph.json", top_n: int = 500, builder=None, export_all: bool = False):
    """
    å°†å›¾å¯¼å‡ºä¸º ECharts å¯ç”¨çš„ JSON
    
    Args:
        G: ç½‘ç»œå›¾
        path: è¾“å‡ºè·¯å¾„
        top_n: å¯¼å‡ºåº¦æ•°æœ€é«˜çš„å‰Nä¸ªèŠ‚ç‚¹ï¼ˆä»…å½“export_all=Falseæ—¶æœ‰æ•ˆï¼‰
        builder: NetworkBuilderå®ä¾‹ï¼Œç”¨äºè·å–èŠ‚ç‚¹è§’è‰²å’Œç”µå½±ä¿¡æ¯
        export_all: æ˜¯å¦å¯¼å‡ºæ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹
    """
    if G.number_of_nodes() == 0:
        print("å›¾ä¸ºç©ºï¼Œæœªå¯¼å‡ºã€‚")
        return

    # é€‰å‡ºèŠ‚ç‚¹
    if export_all:
        # å¯¼å‡ºæ‰€æœ‰èŠ‚ç‚¹
        nodes_list = list(G.nodes())
    else:
        # é€‰å‡ºåº¦æ•°æœ€é«˜çš„ top_n ä¸ªèŠ‚ç‚¹
        degrees = dict(G.degree())
        nodes_list = sorted(degrees, key=lambda x: degrees[x], reverse=True)[:top_n]

    # æ„é€ èŠ‚ç‚¹åˆ—è¡¨
    nodes = []
    for n in nodes_list:
        role = 'unknown'
        movie_count = 0
        avg_rating = 0
        
        if builder and hasattr(builder, 'get_node_role'):
            role = builder.get_node_role(n)
            
        if builder and hasattr(builder, 'persons_dict') and builder.persons_dict:
            if n in builder.persons_dict:
                movies = builder.persons_dict[n]
                movie_count = len(movies)
                if movie_count > 0:
                    total_rating = sum(movie.get('rating', 0) for movie in movies)
                    avg_rating = total_rating / movie_count
        
        nodes.append({
            "id": n,
            "name": n,
            "value": G.degree(n),
            "role": role,
            "movieCount": movie_count,
            "avgRating": avg_rating
        })

    # æ„é€ è¾¹åˆ—è¡¨ - è¿‡æ»¤è‡ªç¯è¾¹
    links = []
    for u, v, data in G.edges(data=True):
        if (u in nodes_list and v in nodes_list) and u != v:
            links.append({
                "source": u,
                "target": v,
                "value": data.get("weight", 1),
                "avgRating": data.get("avg_rating", 0),
                "topMovies": data.get("top_movies", [])
            })

    # å¯¼å‡ºJSON
    data = {"nodes": nodes, "links": links}
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False)

    print(f"å·²å¯¼å‡ºç½‘ç»œæ•°æ®åˆ° {path}ï¼ŒèŠ‚ç‚¹æ•°={len(nodes)}ï¼Œè¾¹æ•°={len(links)}")


def main():
    """ä¸»ç¨‹åº"""
    logger.info("å¼€å§‹å½±äººåˆä½œç½‘ç»œåˆ†æé¡¹ç›®")

    try:
        # ç¬¬ä¸€æ­¥ï¼šåŠ è½½å’Œæ¸…æ´—æ•°æ®
        logger.info("=" * 60)
        logger.info("ç¬¬ä¸€æ­¥ï¼šåŠ è½½å’Œæ¸…æ´—æ•°æ®")
        logger.info("=" * 60)

        loader = DataLoader(EXCEL_FILE)
        df = loader.load_data()
        df = loader.rename_columns()
        df = loader.clean_data()
        df = loader.validate_data()
        cleaned_file = loader.save_cleaned_data('data/cleaned_movies.csv')
        logger.info(f"æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜: {cleaned_file}")

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ•°æ®ï¼Œæå–å½±äººå…³ç³»
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬äºŒæ­¥ï¼šæå–å½±äººå…³ç³»")
        logger.info("=" * 60)

        processor = DataProcessor(df)
        persons_dict = processor.extract_all_persons()
        edges_df = processor.build_cooperation_edges()
        edges_df.to_csv('data/cooperation_edges.csv', index=False, encoding='utf-8-sig')
        logger.info(f"åˆä½œå…³ç³»è¾¹æ•°æ®å·²ä¿å­˜: data/cooperation_edges.csv")

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        person_stats = processor.get_person_stats()
        movie_stats = processor.get_movie_stats()
        person_stats.to_csv('data/person_stats.csv', index=False, encoding='utf-8-sig')
        movie_stats.to_csv('data/movie_stats.csv', index=False, encoding='utf-8-sig')
        logger.info("å½±äººå’Œç”µå½±ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜")

        # ç¬¬ä¸‰æ­¥ï¼šæ„å»ºç½‘ç»œ
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬ä¸‰æ­¥ï¼šæ„å»ºåˆä½œç½‘ç»œ")
        logger.info("=" * 60)

        builder = NetworkBuilder()
        G = builder.build_network(edges_df, persons_dict)
        G_low_rating = builder.extract_low_rating_subnetwork(RATING_THRESHOLD)

        # è·å–é«˜é¢‘åˆä½œå¯¹
        high_coop_pairs = builder.get_high_cooperation_pairs(MIN_COOPERATION_COUNT)
        high_coop_df = pd.DataFrame(high_coop_pairs)
        high_coop_df.to_csv('data/high_cooperation_pairs.csv', index=False, encoding='utf-8-sig')
        logger.info(f"é«˜é¢‘åˆä½œå¯¹å·²ä¿å­˜: data/high_cooperation_pairs.csv")

        # è®¡ç®—ç½‘ç»œæŒ‡æ ‡å’Œç¤¾åŒºæ£€æµ‹
        builder.calculate_network_metrics()
        logger.info(f"ç½‘ç»œæŒ‡æ ‡è®¡ç®—å®Œæˆ")

        try:
            partition = builder.detect_communities(COMMUNITY_DETECTION_METHOD)
        except Exception as e:
            logger.warning(f"ç¤¾åŒºæ£€æµ‹å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨ç¤¾åŒºä¿¡æ¯è¿›è¡Œå¯è§†åŒ–: {e}")

        # ç¬¬å››æ­¥ï¼šç½‘ç»œåˆ†æ
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬å››æ­¥ï¼šç½‘ç»œåˆ†æ")
        logger.info("=" * 60)

        try:
            total_nodes = G.number_of_nodes()
            total_edges = G.number_of_edges()
            density = nx.density(G) if total_nodes > 1 else 0
            avg_degree = sum(d for n, d in G.degree()) / total_nodes if total_nodes > 0 else 0
            
            logger.info(f"ç½‘ç»œåŸºæœ¬ç»“æ„: èŠ‚ç‚¹æ•°={total_nodes}, è¾¹æ•°={total_edges}, å¯†åº¦={density:.4f}, å¹³å‡åº¦={avg_degree:.2f}")
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = "å½±äººåˆä½œç½‘ç»œåˆ†ææŠ¥å‘Š\n" + "="*40 + "\n"
            report += f"ç½‘ç»œåŸºæœ¬ä¿¡æ¯:\n"
            report += f"- æ€»èŠ‚ç‚¹æ•°: {total_nodes}\n"
            report += f"- æ€»è¾¹æ•°: {total_edges}\n"
            report += f"- ç½‘ç»œå¯†åº¦: {density:.4f}\n"
            report += f"- å¹³å‡åº¦: {avg_degree:.2f}\n"
            
            with open('data/analysis_report.txt', 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: data/analysis_report.txt")
            print("\n" + report)
            
        except Exception as e:
            logger.error(f"ç½‘ç»œåˆ†æè¿‡ç¨‹å‡ºé”™: {e}", exc_info=True)
            print(f"\nâŒ ç½‘ç»œåˆ†æè¿‡ç¨‹å‡ºé”™: {e}")

        # ç¬¬äº”æ­¥ï¼šå¯¼å‡ºEChartsæ ¼å¼æ•°æ®ï¼ˆç”¨äºå‰ç«¯å¯è§†åŒ–ï¼‰
        logger.info("\n" + "=" * 60)
        logger.info("ç¬¬äº”æ­¥ï¼šå¯¼å‡ºEChartså¯è§†åŒ–æ•°æ®")
        logger.info("=" * 60)
        
        try:
            if G_low_rating and G_low_rating.number_of_nodes() > 0:
                # å¯¼å‡ºæ›´å¤šèŠ‚ç‚¹ï¼ˆ500ä¸ªï¼‰ä»¥æä¾›æ›´å®Œæ•´çš„æ•°æ®
                export_graph_for_echarts(G_low_rating, path="data/graph_top500.json", top_n=500, builder=builder)
                logger.info("âœ… å·²å¯¼å‡ºä½åˆ†ç”µå½±ç½‘ç»œæ•°æ®åˆ° data/graph_top500.json")
            else:
                logger.warning("âš ï¸ ä½åˆ†ç”µå½±ç½‘ç»œä¸ºç©ºï¼Œè·³è¿‡å¯¼å‡º")
        
            # åŒæ—¶å¯¼å‡ºå®Œæ•´ç½‘ç»œæ•°æ®ç”¨äºå±•ç¤ºæ›´å¤šå…³ç³»
            export_graph_for_echarts(G, path="data/full_network_data.json", top_n=1000, builder=builder)
            logger.info("âœ… å·²å¯¼å‡ºå®Œæ•´ç½‘ç»œæ•°æ®åˆ° data/full_network_data.json")
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºEChartsæ•°æ®å¤±è´¥: {e}", exc_info=True)
            print(f"\nâŒ å¯¼å‡ºEChartsæ•°æ®å¤±è´¥: {e}")

        logger.info("\n" + "=" * 60)
        logger.info("é¡¹ç›®è¿è¡Œå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° outputs/ ç›®å½•")
        logger.info("=" * 60)

        # æ‰“å°å…³é”®å‘ç°
        print("\nâœ¨ å…³é”®å‘ç°æ€»ç»“ âœ¨")
        print("=" * 40)

        # è¾“å‡ºç½‘ç»œå¯†åº¦æ¯”è¾ƒ
        if G_low_rating and G_low_rating.number_of_nodes() > 0:
            density_whole = nx.density(G)
            density_low = nx.density(G_low_rating)

            if density_low > density_whole * 1.2:
                print("ğŸ”´ é‡è¦å‘ç°ï¼šä½åˆ†ç”µå½±ç½‘ç»œå¯†åº¦æ˜¾è‘—é«˜äºæ•´ä½“ç½‘ç»œ")
                print("   è¿™æ„å‘³ç€å­˜åœ¨ç´§å¯†çš„'çƒ‚ç‰‡åœˆå­'ï¼Œè¿™äº›å½±äººç»å¸¸åœ¨ä¸€èµ·åˆ¶ä½œä½åˆ†ç”µå½±")
            elif density_low > density_whole:
                print("ğŸŸ¡ å‘ç°ï¼šä½åˆ†ç”µå½±ç½‘ç»œå¯†åº¦ç•¥é«˜äºæ•´ä½“ç½‘ç»œ")
                print("   å­˜åœ¨ä¸€äº›å°èŒƒå›´çš„é‡å¤åˆä½œæ¨¡å¼")
            else:
                print("ğŸŸ¢ å‘ç°ï¼šä½åˆ†ç”µå½±ç½‘ç»œç›¸å¯¹ç¨€ç–")
                print("   æ²¡æœ‰æ˜æ˜¾çš„'çƒ‚ç‰‡åœˆå­'ç°è±¡")

        # è¾“å‡ºå‰5ä¸ªé«˜é¢‘åˆä½œå¯¹
        if not high_coop_df.empty:
            print(f"\nğŸ¤ é«˜é¢‘åˆä½œå½±äººå¯¹ (åˆä½œæ¬¡æ•° â‰¥ {MIN_COOPERATION_COUNT}):")
            for _, row in high_coop_df.head().iterrows():
                print(f"   {row['person1']} & {row['person2']}: "
                      f"åˆä½œ{row['cooperation_count']}æ¬¡, å¹³å‡è¯„åˆ†{row['avg_rating']:.1f}")

    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    import os

    os.makedirs('data', exist_ok=True)
    os.makedirs('outputs', exist_ok=True)

    main()