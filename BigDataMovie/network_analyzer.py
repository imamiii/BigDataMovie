# network_analyzer.py
import networkx as nx
import pandas as pd
import numpy as np
from collections import Counter
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NetworkAnalyzer:
    def __init__(self, G, G_low_rating=None):
        self.G = G
        self.G_low_rating = G_low_rating

    def analyze_network_structure(self):
        """åˆ†æç½‘ç»œç»“æ„ç‰¹å¾"""
        logger.info("å¼€å§‹åˆ†æç½‘ç»œç»“æ„...")

        analysis_results = {}

        # 1. åŸºæœ¬ç½‘ç»œæŒ‡æ ‡
        logger.info("è®¡ç®—åŸºæœ¬ç½‘ç»œæŒ‡æ ‡...")
        analysis_results['basic_metrics'] = self._calculate_basic_metrics()

        # 2. åº¦åˆ†å¸ƒåˆ†æ
        logger.info("åˆ†æåº¦åˆ†å¸ƒ...")
        analysis_results['degree_distribution'] = self._analyze_degree_distribution()

        # 3. èšç±»ç³»æ•°åˆ†æ
        logger.info("åˆ†æèšç±»ç‰¹æ€§...")
        analysis_results['clustering_analysis'] = self._analyze_clustering()

        # 4. ä¸­å¿ƒæ€§åˆ†æ
        logger.info("åˆ†æä¸­å¿ƒæ€§...")
        analysis_results['centrality_analysis'] = self._analyze_centrality()

        # 5. ä½åˆ†ç½‘ç»œå¯¹æ¯”åˆ†æ
        if self.G_low_rating and self.G_low_rating.number_of_nodes() > 0:
            logger.info("å¯¹æ¯”ä½åˆ†ç½‘ç»œä¸æ•´ä½“ç½‘ç»œ...")
            analysis_results['low_rating_comparison'] = self._compare_low_rating_network()

        logger.info("ç½‘ç»œç»“æ„åˆ†æå®Œæˆ")
        return analysis_results

    def _calculate_basic_metrics(self):
        """è®¡ç®—åŸºæœ¬ç½‘ç»œæŒ‡æ ‡"""
        metrics = {
            'èŠ‚ç‚¹æ•°': self.G.number_of_nodes(),
            'è¾¹æ•°': self.G.number_of_edges(),
            'ç½‘ç»œå¯†åº¦': nx.density(self.G),
            'å¹³å‡è·¯å¾„é•¿åº¦': self._calculate_average_path_length(),
            'å¹³å‡èšç±»ç³»æ•°': nx.average_clustering(self.G)
        }
        
        # åªå¯¹å°ç½‘ç»œè®¡ç®—ç½‘ç»œç›´å¾„ï¼ˆå¤§å‹ç½‘ç»œè®¡ç®—æˆæœ¬å¤ªé«˜ï¼‰
        if self.G.number_of_nodes() < 1000 and nx.is_connected(self.G):
            metrics['ç½‘ç»œç›´å¾„'] = nx.diameter(self.G)
        else:
            metrics['ç½‘ç»œç›´å¾„'] = "ä¸è¿é€šæˆ–è®¡ç®—æˆæœ¬è¿‡é«˜"
        
        return metrics

    def _calculate_average_path_length(self):
        """è®¡ç®—å¹³å‡è·¯å¾„é•¿åº¦"""
        # åªå¯¹å°ç½‘ç»œè®¡ç®—å¹³å‡è·¯å¾„é•¿åº¦ï¼ˆå¤§å‹ç½‘ç»œè®¡ç®—æˆæœ¬å¤ªé«˜ï¼‰
        if self.G.number_of_nodes() < 5000:
            if nx.is_connected(self.G):
                return nx.average_shortest_path_length(self.G)
            else:
                # è®¡ç®—æœ€å¤§è¿é€šåˆ†é‡çš„å¹³å‡è·¯å¾„é•¿åº¦
                largest_component = max(nx.connected_components(self.G), key=len)
                if len(largest_component) < 5000:
                    subgraph = self.G.subgraph(largest_component)
                    return nx.average_shortest_path_length(subgraph)
        return "è®¡ç®—æˆæœ¬è¿‡é«˜"

    def _analyze_degree_distribution(self):
        """åˆ†æåº¦åˆ†å¸ƒ"""
        degrees = [d for n, d in self.G.degree()]

        distribution = {
            'å¹³å‡åº¦': np.mean(degrees),
            'æœ€å¤§åº¦': max(degrees),
            'æœ€å°åº¦': min(degrees),
            'åº¦åˆ†å¸ƒ': Counter(degrees)
        }

        # è®¡ç®—åº¦åˆ†å¸ƒç›´æ–¹å›¾
        hist, bins = np.histogram(degrees, bins=20)
        distribution['ç›´æ–¹å›¾'] = {'é¢‘æ•°': hist.tolist(), 'åŒºé—´': bins.tolist()}

        return distribution

    def _analyze_clustering(self):
        """åˆ†æèšç±»ç‰¹æ€§"""
        # è®¡ç®—å¹³å‡èšç±»ç³»æ•°ï¼ˆæ¯”è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„èšç±»ç³»æ•°æ›´é«˜æ•ˆï¼‰
        avg_clustering = nx.average_clustering(self.G)
        
        # åªå¯¹å°ç½‘ç»œè®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„èšç±»ç³»æ•°
        if self.G.number_of_nodes() < 5000:
            clustering_coeffs = nx.clustering(self.G)
        else:
            logger.info("ç½‘ç»œè¾ƒå¤§ï¼Œåªè®¡ç®—å¹³å‡èšç±»ç³»æ•°")
            clustering_coeffs = None

        analysis = {
            'å¹³å‡èšç±»ç³»æ•°': avg_clustering,
            'é«˜èšç±»èŠ‚ç‚¹': []
        }

        # æ‰¾å‡ºèšç±»ç³»æ•°æœ€é«˜çš„èŠ‚ç‚¹ï¼ˆåªå¯¹å°ç½‘ç»œè®¡ç®—ï¼‰
        if clustering_coeffs is not None:
            sorted_nodes = sorted(clustering_coeffs.items(), key=lambda x: x[1], reverse=True)[:10]
            for node, coeff in sorted_nodes:
                analysis['é«˜èšç±»èŠ‚ç‚¹'].append({'èŠ‚ç‚¹': node, 'èšç±»ç³»æ•°': coeff})

        return analysis

    def _analyze_centrality(self):
        """åˆ†æä¸­å¿ƒæ€§"""
        # åº¦ä¸­å¿ƒæ€§ï¼ˆå¿«é€Ÿè®¡ç®—ï¼‰
        degree_cent = nx.degree_centrality(self.G)
        
        centrality = {
            'åº¦ä¸­å¿ƒæ€§å‰10': sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:10]
        }
        
        # åªå¯¹å°ç½‘ç»œè®¡ç®—æ›´å¤æ‚çš„ä¸­å¿ƒæ€§æŒ‡æ ‡
        if self.G.number_of_nodes() < 1000:
            # ä¸­ä»‹ä¸­å¿ƒæ€§ï¼ˆä½¿ç”¨æ›´å°‘çš„é‡‡æ ·ç‚¹ï¼‰
            betweenness_cent = nx.betweenness_centrality(self.G, k=min(50, self.G.number_of_nodes()))
            centrality['ä¸­ä»‹ä¸­å¿ƒæ€§å‰10'] = sorted(betweenness_cent.items(), key=lambda x: x[1], reverse=True)[:10]
            
            # æ¥è¿‘ä¸­å¿ƒæ€§
            closeness_cent = nx.closeness_centrality(self.G)
            centrality['æ¥è¿‘ä¸­å¿ƒæ€§å‰10'] = sorted(closeness_cent.items(), key=lambda x: x[1], reverse=True)[:10]
        else:
            logger.info("ç½‘ç»œè¾ƒå¤§ï¼Œè·³è¿‡ä¸­ä»‹ä¸­å¿ƒæ€§å’Œæ¥è¿‘ä¸­å¿ƒæ€§è®¡ç®—ä»¥æé«˜æ€§èƒ½")

        return centrality

    def _compare_low_rating_network(self):
        """æ¯”è¾ƒä½åˆ†ç½‘ç»œä¸æ•´ä½“ç½‘ç»œ"""
        if not self.G_low_rating or self.G_low_rating.number_of_nodes() == 0:
            return None

        comparison = {}

        # åŸºæœ¬æŒ‡æ ‡å¯¹æ¯”
        comparison['èŠ‚ç‚¹æ•°å¯¹æ¯”'] = {
            'æ•´ä½“ç½‘ç»œ': self.G.number_of_nodes(),
            'ä½åˆ†ç½‘ç»œ': self.G_low_rating.number_of_nodes(),
            'å æ¯”': self.G_low_rating.number_of_nodes() / self.G.number_of_nodes() * 100
        }

        comparison['è¾¹æ•°å¯¹æ¯”'] = {
            'æ•´ä½“ç½‘ç»œ': self.G.number_of_edges(),
            'ä½åˆ†ç½‘ç»œ': self.G_low_rating.number_of_edges(),
            'å æ¯”': self.G_low_rating.number_of_edges() / self.G.number_of_edges() * 100
        }

        comparison['å¯†åº¦å¯¹æ¯”'] = {
            'æ•´ä½“ç½‘ç»œ': nx.density(self.G),
            'ä½åˆ†ç½‘ç»œ': nx.density(self.G_low_rating)
        }

        comparison['å¹³å‡èšç±»ç³»æ•°å¯¹æ¯”'] = {
            'æ•´ä½“ç½‘ç»œ': nx.average_clustering(self.G),
            'ä½åˆ†ç½‘ç»œ': nx.average_clustering(self.G_low_rating)
        }

        # æ‰¾å‡ºä½åˆ†ç½‘ç»œä¸­çš„æ ¸å¿ƒèŠ‚ç‚¹
        low_degree_cent = nx.degree_centrality(self.G_low_rating)
        comparison['ä½åˆ†ç½‘ç»œæ ¸å¿ƒèŠ‚ç‚¹'] = sorted(low_degree_cent.items(), key=lambda x: x[1], reverse=True)[:10]

        return comparison

    def find_high_risk_clusters(self, min_coop_count=3, min_avg_rating=5.0):
        """æ‰¾å‡ºé«˜é£é™©å½±äººå›¢ç°‡"""
        high_risk_clusters = []

        # è·å–æ‰€æœ‰è¿é€šåˆ†é‡
        if self.G_low_rating and self.G_low_rating.number_of_nodes() > 0:
            components = list(nx.connected_components(self.G_low_rating))

            for i, component in enumerate(components):
                if len(component) >= 3:  # åªè€ƒè™‘3äººä»¥ä¸Šçš„å›¢ç°‡
                    subgraph = self.G_low_rating.subgraph(component)

                    # è®¡ç®—å›¢ç°‡æŒ‡æ ‡
                    cluster_info = {
                        'cluster_id': i + 1,
                        'size': len(component),
                        'density': nx.density(subgraph),
                        'avg_degree': np.mean([d for n, d in subgraph.degree()]),
                        'members': list(component)[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªæˆå‘˜
                    }

                    # è®¡ç®—å¹³å‡è¯„åˆ†
                    ratings = []
                    for u, v, data in subgraph.edges(data=True):
                        ratings.extend([r for r in data['ratings'] if not pd.isna(r)])

                    if ratings:
                        cluster_info['avg_rating'] = np.mean(ratings)
                        cluster_info['rating_count'] = len(ratings)

                        # æ£€æŸ¥æ˜¯å¦ä¸ºé«˜é£é™©å›¢ç°‡
                        if (cluster_info['avg_rating'] < min_avg_rating and
                                len(component) >= 3 and
                                cluster_info['density'] > 0.3):
                            high_risk_clusters.append(cluster_info)

        logger.info(f"æ‰¾åˆ° {len(high_risk_clusters)} ä¸ªé«˜é£é™©å½±äººå›¢ç°‡")
        return high_risk_clusters

    def generate_insights_report(self, analysis_results):
        """ç”Ÿæˆåˆ†ææ´å¯ŸæŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("å½±äººåˆä½œç½‘ç»œåˆ†ææŠ¥å‘Š")
        report.append("=" * 60)

        # åŸºæœ¬æŒ‡æ ‡
        metrics = analysis_results['basic_metrics']
        report.append("\n1. åŸºæœ¬ç½‘ç»œæŒ‡æ ‡:")
        for key, value in metrics.items():
            report.append(f"   {key}: {value}")

        # åº¦åˆ†å¸ƒ
        degree_dist = analysis_results['degree_distribution']
        report.append(f"\n2. åº¦åˆ†å¸ƒåˆ†æ:")
        report.append(f"   å¹³å‡åº¦: {degree_dist['å¹³å‡åº¦']:.2f}")
        report.append(f"   æœ€å¤§åº¦: {degree_dist['æœ€å¤§åº¦']} (æœ€æ´»è·ƒçš„å½±äºº)")

        # ä¸­å¿ƒæ€§åˆ†æ
        centrality = analysis_results['centrality_analysis']
        report.append("\n3. ä¸­å¿ƒæ€§åˆ†æ:")
        report.append("   åº¦ä¸­å¿ƒæ€§æœ€é«˜çš„å½±äºº:")
        for i, (person, score) in enumerate(centrality['åº¦ä¸­å¿ƒæ€§å‰10'][:5], 1):
            report.append(f"     {i}. {person}: {score:.3f}")

        # ä½åˆ†ç½‘ç»œå¯¹æ¯”
        if 'low_rating_comparison' in analysis_results:
            comparison = analysis_results['low_rating_comparison']
            report.append("\n4. ä½åˆ†ç”µå½±ç½‘ç»œåˆ†æ:")

            node_comp = comparison['èŠ‚ç‚¹æ•°å¯¹æ¯”']
            edge_comp = comparison['è¾¹æ•°å¯¹æ¯”']
            density_comp = comparison['å¯†åº¦å¯¹æ¯”']

            report.append(f"   ä½åˆ†ç½‘ç»œåŒ…å« {node_comp['ä½åˆ†ç½‘ç»œ']} ä¸ªå½±äºº "
                          f"({node_comp['å æ¯”']:.1f}% çš„æ•´ä½“ç½‘ç»œ)")
            report.append(f"   ä½åˆ†ç½‘ç»œå¯†åº¦: {density_comp['ä½åˆ†ç½‘ç»œ']:.4f} "
                          f"vs æ•´ä½“å¯†åº¦: {density_comp['æ•´ä½“ç½‘ç»œ']:.4f}")

            if density_comp['ä½åˆ†ç½‘ç»œ'] > density_comp['æ•´ä½“ç½‘ç»œ']:
                report.append("   ğŸ” å‘ç°: ä½åˆ†ç”µå½±ç½‘ç»œå¯†åº¦æ›´é«˜ï¼Œå­˜åœ¨æ˜æ˜¾çš„å›¢ç°‡ç»“æ„ï¼")
            else:
                report.append("   ğŸ” å‘ç°: ä½åˆ†ç”µå½±ç½‘ç»œç›¸å¯¹ç¨€ç–")

        # é«˜é£é™©å›¢ç°‡
        high_risk_clusters = self.find_high_risk_clusters()
        if high_risk_clusters:
            report.append("\n5. é«˜é£é™©å½±äººå›¢ç°‡æ£€æµ‹:")
            for cluster in high_risk_clusters[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                report.append(f"   å›¢ç°‡{cluster['cluster_id']}: "
                              f"{cluster['size']}äºº, "
                              f"å¹³å‡è¯„åˆ†{cluster.get('avg_rating', 0):.1f}, "
                              f"å¯†åº¦{cluster['density']:.3f}")
                report.append(f"       æ ¸å¿ƒæˆå‘˜: {', '.join(cluster['members'][:3])}...")

        return "\n".join(report)