# network_analyzer.py
import networkx as nx
import pandas as pd
import numpy as np
from collections import Counter
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NetworkAnalyzer:
    def __init__(self, G, G_low_rating=None):
        self.G = G
        self.G_low_rating = G_low_rating

    def analyze_network_structure(self):
        logger.info("å¼€å§‹åˆ†æç½‘ç»œç»“æ„...")

        analysis_results = {}

        logger.info("è®¡ç®—åŸºæœ¬ç½‘ç»œæŒ‡æ ‡...")
        analysis_results['basic_metrics'] = self._calculate_basic_metrics()

        logger.info("åˆ†æåº¦åˆ†å¸ƒ...")
        analysis_results['degree_distribution'] = self._analyze_degree_distribution()

        logger.info("åˆ†æèšç±»ç‰¹æ€§...")
        analysis_results['clustering_analysis'] = self._analyze_clustering()

        logger.info("åˆ†æä¸­å¿ƒæ€§...")
        analysis_results['centrality_analysis'] = self._analyze_centrality()

        if self.G_low_rating and self.G_low_rating.number_of_nodes() > 0:
            logger.info("å¯¹æ¯”ä½åˆ†ç½‘ç»œä¸æ•´ä½“ç½‘ç»œ...")
            analysis_results['low_rating_comparison'] = self._compare_low_rating_network()

        logger.info("ç½‘ç»œç»“æ„åˆ†æå®Œæˆ")
        return analysis_results

    def _calculate_basic_metrics(self):
        metrics = {
            'èŠ‚ç‚¹æ•°': self.G.number_of_nodes(),
            'è¾¹æ•°': self.G.number_of_edges(),
            'ç½‘ç»œå¯†åº¦': nx.density(self.G),
            'å¹³å‡è·¯å¾„é•¿åº¦': self._calculate_average_path_length(),
            'å¹³å‡èšç±»ç³»æ•°': nx.average_clustering(self.G)
        }

        if self.G.number_of_nodes() < 1000 and nx.is_connected(self.G):
            metrics['ç½‘ç»œç›´å¾„'] = nx.diameter(self.G)
        else:
            metrics['ç½‘ç»œç›´å¾„'] = "ä¸è¿é€šæˆ–è®¡ç®—æˆæœ¬è¿‡é«˜"

        return metrics

    def _calculate_average_path_length(self):
        if self.G.number_of_nodes() < 5000:
            if nx.is_connected(self.G):
                return nx.average_shortest_path_length(self.G)
            else:
                largest_component = max(nx.connected_components(self.G), key=len)
                if len(largest_component) < 5000:
                    subgraph = self.G.subgraph(largest_component)
                    return nx.average_shortest_path_length(subgraph)
        return "è®¡ç®—æˆæœ¬è¿‡é«˜"

    def _analyze_degree_distribution(self):
        degrees = [d for _, d in self.G.degree()]

        distribution = {
            'å¹³å‡åº¦': np.mean(degrees) if degrees else 0,
            'æœ€å¤§åº¦': max(degrees) if degrees else 0,
            'æœ€å°åº¦': min(degrees) if degrees else 0,
            'åº¦åˆ†å¸ƒ': Counter(degrees)
        }

        hist, bins = np.histogram(degrees, bins=20) if degrees else (np.array([]), np.array([]))
        distribution['ç›´æ–¹å›¾'] = {'é¢‘æ•°': hist.tolist(), 'åŒºé—´': bins.tolist()}

        return distribution

    def _analyze_clustering(self):
        avg_clustering = nx.average_clustering(self.G)

        if self.G.number_of_nodes() < 5000:
            clustering_coeffs = nx.clustering(self.G)
        else:
            logger.info("ç½‘ç»œè¾ƒå¤§ï¼Œåªè®¡ç®—å¹³å‡èšç±»ç³»æ•°")
            clustering_coeffs = None

        analysis = {
            'å¹³å‡èšç±»ç³»æ•°': avg_clustering,
            'é«˜èšç±»èŠ‚ç‚¹': []
        }

        if clustering_coeffs is not None:
            sorted_nodes = sorted(clustering_coeffs.items(), key=lambda x: x[1], reverse=True)[:10]
            for node, coeff in sorted_nodes:
                analysis['é«˜èšç±»èŠ‚ç‚¹'].append({'èŠ‚ç‚¹': node, 'èšç±»ç³»æ•°': coeff})

        return analysis

    def _analyze_centrality(self):
        degree_cent = nx.degree_centrality(self.G)

        centrality = {
            'åº¦ä¸­å¿ƒæ€§å‰10': sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:10]
        }

        if self.G.number_of_nodes() < 1000:
            betweenness_cent = nx.betweenness_centrality(self.G, k=min(50, self.G.number_of_nodes()))
            centrality['ä¸­ä»‹ä¸­å¿ƒæ€§å‰10'] = sorted(betweenness_cent.items(), key=lambda x: x[1], reverse=True)[:10]

            closeness_cent = nx.closeness_centrality(self.G)
            centrality['æ¥è¿‘ä¸­å¿ƒæ€§å‰10'] = sorted(closeness_cent.items(), key=lambda x: x[1], reverse=True)[:10]
        else:
            logger.info("ç½‘ç»œè¾ƒå¤§ï¼Œè·³è¿‡ä¸­ä»‹ä¸­å¿ƒæ€§å’Œæ¥è¿‘ä¸­å¿ƒæ€§è®¡ç®—ä»¥æé«˜æ€§èƒ½")

        return centrality

    def _compare_low_rating_network(self):
        if not self.G_low_rating or self.G_low_rating.number_of_nodes() == 0:
            return None

        comparison = {}

        comparison['èŠ‚ç‚¹æ•°å¯¹æ¯”'] = {
            'æ•´ä½“ç½‘ç»œ': self.G.number_of_nodes(),
            'ä½åˆ†ç½‘ç»œ': self.G_low_rating.number_of_nodes(),
            'å æ¯”': self.G_low_rating.number_of_nodes() / self.G.number_of_nodes() * 100 if self.G.number_of_nodes() else 0
        }

        comparison['è¾¹æ•°å¯¹æ¯”'] = {
            'æ•´ä½“ç½‘ç»œ': self.G.number_of_edges(),
            'ä½åˆ†ç½‘ç»œ': self.G_low_rating.number_of_edges(),
            'å æ¯”': self.G_low_rating.number_of_edges() / self.G.number_of_edges() * 100 if self.G.number_of_edges() else 0
        }

        comparison['å¯†åº¦å¯¹æ¯”'] = {
            'æ•´ä½“ç½‘ç»œ': nx.density(self.G),
            'ä½åˆ†ç½‘ç»œ': nx.density(self.G_low_rating)
        }

        comparison['å¹³å‡èšç±»ç³»æ•°å¯¹æ¯”'] = {
            'æ•´ä½“ç½‘ç»œ': nx.average_clustering(self.G),
            'ä½åˆ†ç½‘ç»œ': nx.average_clustering(self.G_low_rating)
        }

        low_degree_cent = nx.degree_centrality(self.G_low_rating)
        comparison['ä½åˆ†ç½‘ç»œæ ¸å¿ƒèŠ‚ç‚¹'] = sorted(low_degree_cent.items(), key=lambda x: x[1], reverse=True)[:10]

        return comparison

    def find_high_risk_clusters(self, min_coop_count=3, min_avg_rating=5.0):
        """æ‰¾å‡ºé«˜é£é™©å½±äººå›¢ç°‡ï¼ˆå…¼å®¹ä½ çš„è¾¹ç»“æ„ï¼šä» movies / avg_rating é‡Œå–è¯„åˆ†ï¼‰"""
        high_risk_clusters = []

        if self.G_low_rating and self.G_low_rating.number_of_nodes() > 0:
            components = list(nx.connected_components(self.G_low_rating))

            for i, component in enumerate(components):
                if len(component) >= 3:
                    subgraph = self.G_low_rating.subgraph(component)

                    cluster_info = {
                        'cluster_id': i + 1,
                        'size': len(component),
                        'density': nx.density(subgraph),
                        'avg_degree': float(np.mean([d for _, d in subgraph.degree()])),
                        'members': list(component)[:10]
                    }

                    # ä»è¾¹çš„ movies é‡Œæ±‡æ€»è¯„åˆ†ï¼ˆä½ çš„è¾¹æ²¡æœ‰ data['ratings']ï¼‰
                    ratings = []
                    for _, _, data in subgraph.edges(data=True):
                        if "movies" in data and isinstance(data["movies"], list):
                            for m in data["movies"]:
                                r = m.get("rating", None)
                                if r is not None and not pd.isna(r):
                                    ratings.append(float(r))
                        else:
                            r = data.get("avg_rating", None)
                            if r is not None and not pd.isna(r):
                                ratings.append(float(r))

                    if ratings:
                        cluster_info['avg_rating'] = float(np.mean(ratings))
                        cluster_info['rating_count'] = int(len(ratings))

                        if (cluster_info['avg_rating'] < min_avg_rating and
                                len(component) >= 3 and
                                cluster_info['density'] > 0.3):
                            high_risk_clusters.append(cluster_info)

        logger.info(f"æ‰¾åˆ° {len(high_risk_clusters)} ä¸ªé«˜é£é™©å½±äººå›¢ç°‡")
        return high_risk_clusters

    def generate_insights_report(self, analysis_results):
        report = []
        report.append("=" * 60)
        report.append("å½±äººåˆä½œç½‘ç»œåˆ†ææŠ¥å‘Š")
        report.append("=" * 60)

        metrics = analysis_results['basic_metrics']
        report.append("\n1. åŸºæœ¬ç½‘ç»œæŒ‡æ ‡:")
        for key, value in metrics.items():
            report.append(f"   {key}: {value}")

        degree_dist = analysis_results['degree_distribution']
        report.append(f"\n2. åº¦åˆ†å¸ƒåˆ†æ:")
        report.append(f"   å¹³å‡åº¦: {degree_dist['å¹³å‡åº¦']:.2f}")
        report.append(f"   æœ€å¤§åº¦: {degree_dist['æœ€å¤§åº¦']} (æœ€æ´»è·ƒçš„å½±äºº)")

        centrality = analysis_results['centrality_analysis']
        report.append("\n3. ä¸­å¿ƒæ€§åˆ†æ:")
        report.append("   åº¦ä¸­å¿ƒæ€§æœ€é«˜çš„å½±äºº:")
        for i, (person, score) in enumerate(centrality['åº¦ä¸­å¿ƒæ€§å‰10'][:5], 1):
            report.append(f"     {i}. {person}: {score:.3f}")

        if 'low_rating_comparison' in analysis_results:
            comparison = analysis_results['low_rating_comparison']
            report.append("\n4. ä½åˆ†ç”µå½±ç½‘ç»œåˆ†æ:")

            node_comp = comparison['èŠ‚ç‚¹æ•°å¯¹æ¯”']
            density_comp = comparison['å¯†åº¦å¯¹æ¯”']

            report.append(f"   ä½åˆ†ç½‘ç»œåŒ…å« {node_comp['ä½åˆ†ç½‘ç»œ']} ä¸ªå½±äºº "
                          f"({node_comp['å æ¯”']:.1f}% çš„æ•´ä½“ç½‘ç»œ)")
            report.append(f"   ä½åˆ†ç½‘ç»œå¯†åº¦: {density_comp['ä½åˆ†ç½‘ç»œ']:.4f} "
                          f"vs æ•´ä½“å¯†åº¦: {density_comp['æ•´ä½“ç½‘ç»œ']:.4f}")

            if density_comp['ä½åˆ†ç½‘ç»œ'] > density_comp['æ•´ä½“ç½‘ç»œ']:
                report.append("   ğŸ” å‘ç°: ä½åˆ†ç”µå½±ç½‘ç»œå¯†åº¦æ›´é«˜ï¼Œå­˜åœ¨æ˜æ˜¾çš„å›¢ç°‡ç»“æ„ï¼")
            else:
                report.append("   ğŸ” å‘ç°: ä½åˆ†ç”µå½±ç½‘ç»œç›¸å¯¹ç¨€ç–")

        high_risk_clusters = self.find_high_risk_clusters()
        if high_risk_clusters:
            report.append("\n5. é«˜é£é™©å½±äººå›¢ç°‡æ£€æµ‹:")
            for cluster in high_risk_clusters[:5]:
                report.append(f"   å›¢ç°‡{cluster['cluster_id']}: "
                              f"{cluster['size']}äºº, "
                              f"å¹³å‡è¯„åˆ†{cluster.get('avg_rating', 0):.1f}, "
                              f"å¯†åº¦{cluster['density']:.3f}")
                report.append(f"       æ ¸å¿ƒæˆå‘˜: {', '.join(cluster['members'][:3])}...")

        return "\n".join(report)